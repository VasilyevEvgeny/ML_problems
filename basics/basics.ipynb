{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0\n"
     ]
    }
   ],
   "source": [
    "def power_sum(l, r, p=1.0):\n",
    "    \"\"\"\n",
    "        input: l, r - integers, p - float\n",
    "        returns sum of p-powers of integers from [l, r]\n",
    "        \n",
    "        example: power_sum(2, 4, 2.0) == 2 ** 2.0 + 3 ** 2.0 + 4 ** 2.0 = 4.0 + 9.0 + 16.0 = 29.0\n",
    "    \"\"\"\n",
    "    return sum([i**p for i in range(l, r+1)])\n",
    "\n",
    "print(power_sum(2, 4, 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def solve_equation(a, b, c):\n",
    "    \"\"\"\n",
    "        input: a, b, c - integers\n",
    "        returns float solutions x of the following equation: a x ** 2 + b x + c == 0\n",
    "            In case of two diffrent solution returns tuple / list (x1, x2)\n",
    "            In case of one solution returns one float\n",
    "            In case of no float solutions return None \n",
    "            In case of infinity number of solutions returns 'inf'\n",
    "    \"\"\"\n",
    "    D = b**2 - 4.*a*c\n",
    "    if a or b or c:\n",
    "        if a:\n",
    "            if D > 0:\n",
    "                return ((-b-np.sqrt(D))/(2*a), (-b+np.sqrt(D))/(2*a))\n",
    "            elif D == 0:\n",
    "                return -b/(2*a)\n",
    "            else:\n",
    "                return None\n",
    "        elif b:\n",
    "            return -c / float(b)\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return np.inf\n",
    "\n",
    "print(solve_equation(0, 0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03552231  0.03552231  0.03552231  0.03552231  0.03552231  0.03552231\n",
      "  0.03552231  0.03552231  0.03552231  0.03552231]\n"
     ]
    }
   ],
   "source": [
    "def replace_outliers(x, std_mul=3.0):\n",
    "    \"\"\"\n",
    "        input: x - numpy vector, std_mul - positive float\n",
    "        returns copy of x with all outliers (elements, which are beyond std_mul * (standart deviation) from mean)\n",
    "        replaced with mean  \n",
    "    \"\"\"\n",
    "    mean, std = np.mean(x), np.std(x)\n",
    "    left_boundary, right_boundary = mean - std_mul * std, mean + std_mul * std\n",
    "    return np.array([ mean if element < left_boundary or element > right_boundary else element for element in x ])\n",
    "\n",
    "x = np.random.normal(0, 2, 10000)\n",
    "x.sort()\n",
    "print(replace_outliers(x)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.89442719  0.83205029]\n"
     ]
    }
   ],
   "source": [
    "def get_eigenvector(A, alpha):\n",
    "    \"\"\"\n",
    "        input: A - square numpy matrix, alpha - float\n",
    "        returns numpy vector - any eigenvector of A corresponding to eigenvalue alpha, \n",
    "                or None if alpha is not an eigenvalue.\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenmatrix = np.linalg.eig(A)\n",
    "    if alpha in eigenvalues:\n",
    "        return eigenmatrix[:,0][0]\n",
    "    else:\n",
    "        return None\n",
    "matr = [[-1., -6.],\n",
    "       [ 2.,  6.]],\n",
    "print(get_eigenvector(matr, 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1996 0.8004\n"
     ]
    }
   ],
   "source": [
    "def discrete_sampler(p):\n",
    "    \"\"\"\n",
    "        input: p - numpy vector of probability (non-negative, sums to 1)\n",
    "        returns integer from 0 to len(p) - 1, each integer i is returned with probability p[i] \n",
    "    \"\"\"\n",
    "    return np.random.choice(a=range(len(p)), size=1, p=p)[0]\n",
    "\n",
    "p = [0.2, 0.8]\n",
    "pull = []\n",
    "MAX = 10000\n",
    "for i in range(MAX):\n",
    "    pull.append(discrete_sampler(p))\n",
    "\n",
    "print(pull.count(0)/MAX, pull.count(1)/MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485.702451274\n"
     ]
    }
   ],
   "source": [
    "def gaussian_log_likelihood(x, mu=0.0, sigma=1.0):\n",
    "    \"\"\"\n",
    "        input: x - numpy vector, mu - float, sigma - positive float\n",
    "        returns log p(x| mu, sigma) - log-likelihood of x dataset \n",
    "        in univariate gaussian model with mean mu and standart deviation sigma\n",
    "    \"\"\"\n",
    "    return -sum([ (xi-mu)**2 for xi in x ]) / (2*sigma**2) + 0.5*len(x)*np.log(2*np.pi*sigma**2)\n",
    "\n",
    "x = np.random.normal(-0.1, 0.9, 1000)\n",
    "print(gaussian_log_likelihood(x, mu=0.0, sigma=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28366218  0.28366218]\n"
     ]
    }
   ],
   "source": [
    "def gradient_approx(f, x0, eps=1e-8):\n",
    "    \"\"\"\n",
    "        input: f - callable, function of vector x. x0 - numpy vector, eps - float, represents step for x_i\n",
    "        returns numpy vector - gradient of f in x0 calculated with finite difference method \n",
    "        (for reference use https://en.wikipedia.org/wiki/Numerical_differentiation, search for \"first-order divided difference\")\n",
    "    \"\"\"\n",
    "    return (f(x0+eps) - f(x0))/eps\n",
    "\n",
    "x0 = np.array([5, 5])\n",
    "print(gradient_approx(np.sin, x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.2756071493751382e-15, array([ 1.,  1.]))\n"
     ]
    }
   ],
   "source": [
    "def gradient_method(f, x0, n_steps=1000, learning_rate=1e-4, eps=1e-8):\n",
    "    \"\"\"\n",
    "        input: f - function of x. x0 - numpy vector, n_steps - integer, learning rate, eps - float.\n",
    "        returns tuple (f^*, x^*), where x^* is local minimum point, found after n_steps of gradient descent, \n",
    "                                        f^* - resulting function value.\n",
    "        Impletent gradient descent method, given in the lecture. \n",
    "        For gradient use finite difference approximation with eps step.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for epoch in range(n_steps):\n",
    "        x = x - learning_rate * gradient_approx(f, x, eps)\n",
    "        \n",
    "    return (f(x), x)\n",
    "\n",
    "def rosen(x):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2)**2 + (1-x[:-1])**2)\n",
    "               \n",
    "x0 = np.array([1.5, 1.5])\n",
    "print(gradient_method(rosen, x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_predict(w, b, X):\n",
    "    \"\"\"\n",
    "        input: w - numpy vector of M weights, b - bias, X - numpy matrix N x M (object-feature matrix), \n",
    "        N - number of objects, M - number of features.\n",
    "        returns numpy vector of predictions of linear regression model for X\n",
    "        https://xkcd.com/1725/\n",
    "    \"\"\"\n",
    "    return w.reshape(1, -1).dot(X) + b.reshape(1,-1)\n",
    "\n",
    "w = np.array([1,2,3])\n",
    "X = np.array([[1, 1, 1, 1],\\\n",
    "             [1, 1, 1, 1],\\\n",
    "             [1, 1, 1, 1]])\n",
    "b = np.array([1,1,1,1])\n",
    "             \n",
    "print(linear_regression_predict(w, b, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        input: two numpy vectors of object targets and model predictions.\n",
    "        return mse\n",
    "    \"\"\"\n",
    "    return sum((y_true-y_pred)**2) / len(y_true)\n",
    "\n",
    "y_true = np.array([1,2,3])\n",
    "y_pred = np.array([4,5,6])\n",
    "print(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  6.  6.]\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_mse_gradient(w, b, X, y_true):\n",
    "    \"\"\"\n",
    "        input: w, b - weights and bias of a linear regression model,\n",
    "                X - object-feature matrix, y_true - targets.\n",
    "        returns gradient of linear regression model mean squared error w.r.t (with respect to) w and b\n",
    "    \"\"\"\n",
    "    y_pred = linear_regression_predict(w, b, X)\n",
    "    y_true = y_true.reshape(1,-1)\n",
    "    n = y_true.shape[1]\n",
    "    return np.array([ sum([ (y_pred[0,j] - y_true[0,j]) * X[i,j] for j in range(n) ]) / n for i in range(X.shape[0]) ])\n",
    "\n",
    "w = np.array([1,2,3])\n",
    "X = np.array([[1, 1, 1, 1],\\\n",
    "             [1, 1, 1, 1],\\\n",
    "             [1, 1, 1, 1]])\n",
    "b = np.array([1,1,1,1])\n",
    "y_true = np.array([1,1,1,1])\n",
    "print(linear_regression_mse_gradient(w, b, X, y_true))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
